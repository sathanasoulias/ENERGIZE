{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ENERGIZE NILM — Post-Training Static Quantization (PTQ)\n",
    "\n",
    "This notebook loads a **fine-tuned pruned model** (output of `03_pruning.ipynb`) and applies\n",
    "**Static Post-Training Quantization** using PyTorch's `torch.ao.quantization` with the\n",
    "`fbgemm` backend (x86 CPU).\n",
    "\n",
    "Pipeline:\n",
    "1. **Configure** — choose model, appliance and pruning ratio (must match `03_pruning.ipynb`)\n",
    "2. **Load data** — same split used for pruning\n",
    "3. **Rebuild pruned model** — fresh baseline → re-apply pruning → load fine-tuned weights\n",
    "4. **Apply Static PTQ** — insert observers → calibrate → convert to INT8\n",
    "5. **Evaluate** — test-set metrics on the quantized model\n",
    "6. **Export** — append quantized row to existing Excel, save quantized model\n",
    "\n",
    "> Pruning / evaluation utilities live in `src_pytorch/pruner.py`.\n",
    "\n",
    "---\n",
    "\n",
    "## Google Colab Setup\n",
    "1. Upload your `ENERGIZE` folder to Google Drive\n",
    "2. Run the cell below first and edit `DRIVE_PROJECT_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLAB SETUP — run this cell first\n",
    "# ============================================================================\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'torch_pruning', 'openpyxl'])\n",
    "\n",
    "    # =========================================================================\n",
    "    DRIVE_PROJECT_PATH = '/content/drive/MyDrive/ENERGIZE'  # <-- EDIT THIS\n",
    "    # =========================================================================\n",
    "\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    project_root = Path(DRIVE_PROJECT_PATH)\n",
    "    if not project_root.exists():\n",
    "        raise FileNotFoundError(f\"Project folder not found: {project_root}\")\n",
    "    os.chdir(project_root)\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Project root: {project_root}\")\n",
    "else:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    project_root = Path(os.getcwd()).parent\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Running locally. Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization as tq\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NILM package\n",
    "from src_pytorch import (\n",
    "    CNN_NILM, TCN_NILM,\n",
    "    SimpleNILMDataLoader,\n",
    "    set_seeds, get_device,\n",
    "    apply_torch_pruning,\n",
    "    get_model_stats,\n",
    "    evaluate_model,\n",
    ")\n",
    "\n",
    "set_seeds(42)\n",
    "device = get_device()\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Device          : {device}\")\n",
    "print(f\"Quantized engine available: fbgemm\" if hasattr(torch.backends, 'quantized') else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Edit only this cell.** The `MODEL_NAME`, `APPLIANCE_NAME` and `PRUNING_RATIO` must match\n",
    "what was used in `03_pruning.ipynb` so the correct fine-tuned checkpoint is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USER CONFIGURATION  (must match 03_pruning.ipynb settings)\n",
    "# ============================================================================\n",
    "MODEL_NAME      = 'tcn'      # 'cnn'  |  'tcn'\n",
    "APPLIANCE_NAME  = 'boiler'   # 'boiler'  |  'ac_1'  |  'washing_machine'\n",
    "PRUNING_RATIO   = 0.5        # must match the pruning ratio used in 03_pruning.ipynb\n",
    "N_CALIB_BATCHES = 100        # number of training batches used for PTQ calibration\n",
    "DATASET_NAME    = 'plegma'\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-DERIVED — do not edit below this line\n",
    "# ============================================================================\n",
    "\n",
    "_MODEL_CFGS = {\n",
    "    'cnn': {\n",
    "        'window'          : 299,\n",
    "        'batch_size'      : 1024,\n",
    "        'args_window_size': 1,\n",
    "    },\n",
    "    'tcn': {\n",
    "        'window'          : 600,\n",
    "        'batch_size'      : 50,\n",
    "        'depth'           : 9,\n",
    "        'filters'         : [512, 256, 256, 128, 128, 256, 256, 256, 512],\n",
    "        'dropout'         : 0.2,\n",
    "        'stacks'          : 1,\n",
    "        'args_window_size': 600,\n",
    "    },\n",
    "}\n",
    "\n",
    "_APPLIANCE_CFGS = {\n",
    "    'boiler'          : {'threshold': 50, 'cutoff': 5000},\n",
    "    'ac_1'            : {'threshold': 50, 'cutoff': 2300},\n",
    "    'washing_machine' : {'threshold': 50, 'cutoff': 2600},\n",
    "}\n",
    "\n",
    "cfg     = _MODEL_CFGS[MODEL_NAME]\n",
    "app_cfg = _APPLIANCE_CFGS[APPLIANCE_NAME]\n",
    "\n",
    "WINDOW     = cfg['window']\n",
    "BATCH_SIZE = cfg['batch_size']\n",
    "THRESHOLD  = app_cfg['threshold']\n",
    "CUTOFF     = app_cfg['cutoff']\n",
    "pct        = int(PRUNING_RATIO * 100)\n",
    "\n",
    "DATA_DIR    = project_root / 'data' / 'processed' / DATASET_NAME / APPLIANCE_NAME\n",
    "CKPT_PATH   = project_root / 'outputs' / f'{MODEL_NAME}_{APPLIANCE_NAME}' / 'checkpoint' / 'model.pt'\n",
    "RESULTS_DIR = project_root / 'outputs' / f'{MODEL_NAME}_{APPLIANCE_NAME}' / 'comparative_results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "finetuned_ckpt = RESULTS_DIR / f'{MODEL_NAME}_{APPLIANCE_NAME}_pruned_{pct}pct_finetuned.pt'\n",
    "\n",
    "print(f\"Model          : {MODEL_NAME.upper()}\")\n",
    "print(f\"Appliance      : {APPLIANCE_NAME}\")\n",
    "print(f\"Pruning ratio  : {pct}%\")\n",
    "print(f\"Calib batches  : {N_CALIB_BATCHES}\")\n",
    "print(f\"Window         : {WINDOW}\")\n",
    "print(f\"Threshold      : {THRESHOLD} W  |  Cutoff: {CUTOFF} W\")\n",
    "print(f\"Baseline ckpt  : {CKPT_PATH}  ({'found' if CKPT_PATH.exists() else 'NOT FOUND'})\")\n",
    "print(f\"FT pruned ckpt : {finetuned_ckpt}  ({'found' if finetuned_ckpt.exists() else 'NOT FOUND'})\")\n",
    "print(f\"Results dir    : {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = SimpleNILMDataLoader(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    model_name=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_window_length=WINDOW,\n",
    "    train=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches : {len(data_loader.train)}\")\n",
    "print(f\"Val   batches : {len(data_loader.val)}\")\n",
    "print(f\"Test  batches : {len(data_loader.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Helper — Model Factory\n",
    "\n",
    "`rebuild_pruned_model` reproduces the pruned architecture deterministically:\n",
    "1. Build a fresh baseline model and load the original checkpoint\n",
    "2. Re-apply the same pruning (magnitude-based, same ratio → same channels removed)\n",
    "3. Replace the weights with the fine-tuned checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name: str, cfg: dict, ckpt_path, target_device) -> nn.Module:\n",
    "    \"\"\"Instantiate and load a NILM model from a checkpoint.\"\"\"\n",
    "    if model_name == 'cnn':\n",
    "        model = CNN_NILM(input_window_length=cfg['window'])\n",
    "    elif model_name == 'tcn':\n",
    "        model = TCN_NILM(\n",
    "            input_window_length=cfg['window'],\n",
    "            depth=cfg['depth'],\n",
    "            nb_filters=cfg['filters'],\n",
    "            dropout=cfg['dropout'],\n",
    "            stacks=cfg['stacks'],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=target_device))\n",
    "    return model.to(target_device).eval()\n",
    "\n",
    "\n",
    "def rebuild_pruned_model(model_name, cfg, baseline_ckpt, pruning_ratio, finetuned_ckpt_path, target_device):\n",
    "    \"\"\"\n",
    "    Rebuild a pruned architecture and load fine-tuned weights.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Instantiate baseline model with original weights\n",
    "    2. Re-apply the same structured pruning (deterministic: same weights → same channels pruned)\n",
    "    3. Load the fine-tuned state_dict into the pruned architecture\n",
    "    \"\"\"\n",
    "    if not finetuned_ckpt_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Fine-tuned checkpoint not found:\\n  {finetuned_ckpt_path}\\n\"\n",
    "            \"Run 03_pruning.ipynb first with matching MODEL_NAME / APPLIANCE_NAME / PRUNING_RATIO.\"\n",
    "        )\n",
    "\n",
    "    # Step 1: fresh baseline\n",
    "    model = build_model(model_name, cfg, baseline_ckpt, target_device)\n",
    "\n",
    "    # Step 2: re-apply pruning to reproduce the pruned architecture\n",
    "    pruning_args = SimpleNamespace(window_size=cfg['args_window_size'])\n",
    "    dummy        = torch.randn(1, cfg['window']).to(target_device)\n",
    "    model, _     = apply_torch_pruning(model, pruning_args, dummy, pruning_ratio)\n",
    "\n",
    "    # Step 3: load fine-tuned weights\n",
    "    model.load_state_dict(torch.load(finetuned_ckpt_path, map_location=target_device))\n",
    "    return model.eval()\n",
    "\n",
    "\n",
    "print(\"build_model() and rebuild_pruned_model() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Load Fine-tuned Pruned Model\n",
    "\n",
    "The quantization pipeline starts from the fine-tuned pruned model.\n",
    "The `fbgemm` backend requires the model to run on **CPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbgemm (x86) requires CPU\n",
    "quant_device = torch.device('cpu')\n",
    "\n",
    "print(\"Rebuilding pruned model and loading fine-tuned weights...\")\n",
    "quant_model = rebuild_pruned_model(\n",
    "    MODEL_NAME, cfg, CKPT_PATH, PRUNING_RATIO, finetuned_ckpt, quant_device\n",
    ")\n",
    "\n",
    "# Record model cost *before* quantization (architecture is unchanged by PTQ)\n",
    "dummy_input   = torch.randn(1, WINDOW).to(quant_device)\n",
    "pruned_params, pruned_macs, pruned_mb = get_model_stats(quant_model, dummy_input)\n",
    "\n",
    "print(f\"\\nPruned model loaded (pre-quantization):\")\n",
    "print(f\"  Parameters : {pruned_params:,}\")\n",
    "print(f\"  MACs       : {pruned_macs:,}\")\n",
    "print(f\"  Size       : {pruned_mb:.3f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Apply Static Post-Training Quantization (PTQ)\n",
    "\n",
    "Three-step process:\n",
    "1. **Prepare** — insert activation observers into the model\n",
    "2. **Calibrate** — run forward passes to collect activation statistics\n",
    "3. **Convert** — replace float ops with INT8 quantized ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "def _fix_string_padding(model: nn.Module) -> None:\n    \"\"\"Replace Conv1d string padding ('same'/'valid') with integers.\n    The quantization C++ backend requires integer padding, not strings.\n    \"\"\"\n    for m in model.modules():\n        if isinstance(m, nn.Conv1d) and isinstance(m.padding, str):\n            if m.padding == 'valid':\n                m.padding = (0,)\n            elif m.padding == 'same':\n                k, d = m.kernel_size[0], m.dilation[0]\n                m.padding = ((k - 1) * d // 2,)\n\n\nclass QuantWrapper(nn.Module):\n    \"\"\"\n    Wraps a float model with explicit QuantStub / DeQuantStub boundaries.\n\n    QuantStub  — converts float input tensor to a quantized tensor\n    DeQuantStub — converts the quantized output back to float\n\n    This defines a clean quantization domain for eager-mode PTQ without\n    symbolic tracing, so models with control flow (if x.dim() == ...) work.\n    \"\"\"\n    def __init__(self, base: nn.Module):\n        super().__init__()\n        self.quant   = tq.QuantStub()\n        self.base    = base\n        self.dequant = tq.DeQuantStub()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.quant(x)      # float → quantized\n        x = self.base(x)       # model runs on QuantizedCPU\n        x = self.dequant(x)    # quantized → float (eval code receives floats)\n        return x\n\n\n# Patch string padding before wrapping\n_fix_string_padding(quant_model)\n\n# --- Step 1: Wrap and configure ---\ntorch.backends.quantized.engine = \"onednn\"\n\nwrapped = QuantWrapper(quant_model).eval()\nwrapped.qconfig = tq.get_default_qconfig(\"onednn\")\n\n# --- Step 2: Prepare (eager mode — no tracing, no TraceError) ---\ntq.prepare(wrapped, inplace=True)\nprint(\"Observers inserted.\")\n\n# --- Step 3: Calibrate ---\nprint(f\"Calibrating on up to {N_CALIB_BATCHES} training batches...\")\nwith torch.inference_mode():\n    for i, (x, _) in enumerate(tqdm(data_loader.train, desc=\"Calibration\")):\n        wrapped(x.to(quant_device))\n        if i >= N_CALIB_BATCHES:\n            break\n\n# --- Step 4: Convert to INT8 ---\ntq.convert(wrapped, inplace=True)\nquant_model = wrapped   # rest of the notebook keeps using quant_model\nprint(\"\\nStatic quantization applied — model converted to INT8.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Evaluate Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Evaluating quantized {MODEL_NAME.upper()} on test set...\")\n",
    "quantized_metrics = evaluate_model(\n",
    "    model=quant_model,\n",
    "    data_loader=data_loader,\n",
    "    model_name=MODEL_NAME,\n",
    "    cutoff=CUTOFF,\n",
    "    threshold=THRESHOLD,\n",
    "    device=quant_device,\n",
    "    input_window_length=WINDOW,\n",
    ")\n",
    "\n",
    "print(f\"\\nQuantized Model Results:\")\n",
    "for k, v in quantized_metrics.items():\n",
    "    print(f\"  {k:<25}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Save Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "quant_ckpt = RESULTS_DIR / f'{MODEL_NAME}_{APPLIANCE_NAME}_pruned_{pct}pct_finetuned_quantized.pt'\n\n# convert_fx returns a GraphModule — save the full model object (not state_dict),\n# because the quantized graph structure is required to reload it correctly.\ntorch.save(quant_model, quant_ckpt)\n\n# Measure on-disk size — captures the actual INT8 weight representation\nquant_mb = round(quant_ckpt.stat().st_size / (1024 ** 2), 3)\n\nprint(f\"Quantized model saved : {quant_ckpt}\")\nprint(f\"On-disk size          : {quant_mb:.3f} MB  (vs {pruned_mb:.3f} MB before quantization)\")\nprint(f\"Size reduction        : {(1 - quant_mb / pruned_mb) * 100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Update Results Excel\n",
    "\n",
    "A new row for the quantized model is appended to the existing\n",
    "`{model}_{appliance}_pruning_results.xlsx` created by `03_pruning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = RESULTS_DIR / f'{MODEL_NAME}_{APPLIANCE_NAME}_pruning_results.xlsx'\n",
    "\n",
    "new_row = {\n",
    "    'Model'           : f'{MODEL_NAME.upper()} Pruned {pct}% + FT + Quantized',\n",
    "    'Architecture'    : MODEL_NAME.upper(),\n",
    "    'Appliance'       : APPLIANCE_NAME,\n",
    "    'Pruning_Ratio_%' : pct,\n",
    "    'Params'          : pruned_params,\n",
    "    'MACs'            : pruned_macs,\n",
    "    'MB'              : quant_mb,\n",
    "    'MAE'             : round(quantized_metrics['mae'],                  4),\n",
    "    'F1'              : round(quantized_metrics['f1'],                   4),\n",
    "    'Precision'       : round(quantized_metrics['precision'],            4),\n",
    "    'Recall'          : round(quantized_metrics['recall'],               4),\n",
    "    'Accuracy'        : round(quantized_metrics['accuracy'],             4),\n",
    "    'Energy_Error_%'  : round(quantized_metrics['energy_error_percent'], 2),\n",
    "}\n",
    "\n",
    "if excel_path.exists():\n",
    "    existing_df = pd.read_excel(excel_path)\n",
    "    updated_df  = pd.concat([existing_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    print(f\"Appending quantized row to existing Excel: {excel_path}\")\n",
    "else:\n",
    "    updated_df = pd.DataFrame([new_row])\n",
    "    print(f\"Excel not found — creating new file: {excel_path}\")\n",
    "\n",
    "updated_df.to_excel(excel_path, index=False)\n",
    "print(f\"Excel updated: {excel_path}\")\n",
    "updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, W = 22, 18\n",
    "SEP  = \"=\" * (C + W * 2 + 2)\n",
    "sep  = \"-\" * (C + W * 2 + 2)\n",
    "hfmt = f\"{{:<{C}}} {{:>{W}}} {{:>{W}}}\"\n",
    "rfmt = f\"{{:<{C}}} {{:>{W}}} {{:>{W}}}\"\n",
    "\n",
    "col_pruned_ft = f\"Pruned {pct}% + FT\"\n",
    "col_quantized = f\"+ Quantized (INT8)\"\n",
    "\n",
    "print(SEP)\n",
    "print(f\"QUANTIZATION SUMMARY  |  {MODEL_NAME.upper()}  |  {APPLIANCE_NAME}  |  pruning={pct}%\")\n",
    "print(SEP)\n",
    "print(hfmt.format('Metric', col_pruned_ft, col_quantized))\n",
    "print(sep)\n",
    "\n",
    "# Recover fine-tuned metrics from Excel for comparison\n",
    "ft_label = f'{MODEL_NAME.upper()} Pruned {pct}% + Fine-tuned 1ep'\n",
    "if excel_path.exists():\n",
    "    _df = pd.read_excel(excel_path)\n",
    "    _ft = _df[_df['Model'] == ft_label]\n",
    "    if not _ft.empty:\n",
    "        _ft = _ft.iloc[0]\n",
    "        ft_mae  = _ft['MAE']\n",
    "        ft_f1   = _ft['F1']\n",
    "        ft_prec = _ft['Precision']\n",
    "        ft_rec  = _ft['Recall']\n",
    "        ft_acc  = _ft['Accuracy']\n",
    "        ft_ee   = _ft['Energy_Error_%']\n",
    "        ft_mb   = _ft['MB']\n",
    "    else:\n",
    "        ft_mae = ft_f1 = ft_prec = ft_rec = ft_acc = ft_ee = ft_mb = float('nan')\n",
    "else:\n",
    "    ft_mae = ft_f1 = ft_prec = ft_rec = ft_acc = ft_ee = ft_mb = float('nan')\n",
    "\n",
    "def rrow(label, a, b):\n",
    "    print(rfmt.format(label, str(a), str(b)))\n",
    "\n",
    "rrow('MB',           f\"{ft_mb:.3f}\",              f\"{quant_mb:.3f}\")\n",
    "print(sep)\n",
    "rrow('MAE (W)',      f\"{ft_mae:.4f}\",              f\"{quantized_metrics['mae']:.4f}\")\n",
    "rrow('F1',           f\"{ft_f1:.4f}\",               f\"{quantized_metrics['f1']:.4f}\")\n",
    "rrow('Precision',    f\"{ft_prec:.4f}\",             f\"{quantized_metrics['precision']:.4f}\")\n",
    "rrow('Recall',       f\"{ft_rec:.4f}\",              f\"{quantized_metrics['recall']:.4f}\")\n",
    "rrow('Accuracy',     f\"{ft_acc:.4f}\",              f\"{quantized_metrics['accuracy']:.4f}\")\n",
    "rrow('Energy Err %', f\"{ft_ee:.2f}\",               f\"{quantized_metrics['energy_error_percent']:.2f}\")\n",
    "print(SEP)\n",
    "print(f\"Excel  : {excel_path}\")\n",
    "print(f\"Ckpt   : {quant_ckpt.name}\")\n",
    "print(SEP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}