{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OpenNILM - Model Evaluation (PyTorch)\n\nThis notebook covers:\n1. **Model Loading**: Load trained model from checkpoint\n2. **Inference**: Generate predictions on test data\n3. **Metrics Calculation**: MAE, F1, Precision, Recall, Accuracy\n4. **Visualization**: Prediction plots and confusion analysis\n5. **Error Analysis**: Detailed error distribution analysis\n\n---\n\n## Google Colab Setup\n\n**Before running this notebook on Colab:**\n1. Upload your OpenNILM folder to Google Drive\n2. Make sure training is complete and checkpoint exists\n3. Run the setup cells below\n\n---"
  },
  {
   "cell_type": "markdown",
   "source": "## 0. Google Colab Setup (Run these cells first!)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# COLAB DETECTION AND SETUP\n# ============================================================================\nimport sys\n\n# Detect if running on Google Colab\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running on Google Colab\")\n    print(\"  -> Please run the following cells to set up your environment\")\nelse:\n    print(\"Running locally\")\n    print(\"  -> You can skip the Colab setup cells below\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# MOUNT GOOGLE DRIVE (Colab only)\n# ============================================================================\nif IN_COLAB:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    print(\"Google Drive mounted at /content/drive\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# INSTALL DEPENDENCIES (Colab only)\n# ============================================================================\nif IN_COLAB:\n    !pip install -q hydra-core omegaconf\n    print(\"Dependencies installed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# SET PROJECT PATH\n# ============================================================================\nimport os\nfrom pathlib import Path\n\nif IN_COLAB:\n    # =========================================================================\n    # CONFIGURE YOUR GOOGLE DRIVE PATH HERE\n    # =========================================================================\n    # Change this to match where you uploaded the OpenNILM folder in Google Drive\n    \n    DRIVE_PROJECT_PATH = '/content/drive/MyDrive/OpenNILM'  # <-- EDIT THIS PATH\n    \n    # =========================================================================\n    \n    project_root = Path(DRIVE_PROJECT_PATH)\n    \n    if not project_root.exists():\n        print(f\"ERROR: Project folder not found at: {project_root}\")\n        print(f\"\\nPlease either:\")\n        print(f\"  1. Upload your OpenNILM folder to Google Drive\")\n        print(f\"  2. OR edit DRIVE_PROJECT_PATH above to match your folder location\")\n    else:\n        print(f\"Project folder found at: {project_root}\")\n        os.chdir(project_root)\n        print(f\"Changed working directory to: {os.getcwd()}\")\nelse:\n    # Local setup\n    project_root = Path(os.getcwd()).parent\n    print(f\"Project root: {project_root}\")\n\n# Add src_pytorch to Python path\nsys.path.insert(0, str(project_root))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# IMPORTS\n# ============================================================================\n# Note: os, sys, Path, and project_root are already set up in the Colab setup cells above\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import (\n    mean_absolute_error, \n    confusion_matrix, \n    classification_report,\n    ConfusionMatrixDisplay\n)\nfrom tqdm import tqdm\n\n# PyTorch NILM modules\nfrom src_pytorch import (\n    CNN_NILM, GRU_NILM, TCN_NILM, get_model,\n    SimpleNILMDataLoader,\n    SimpleTester,\n    set_seeds, get_device, count_parameters\n)\n\n# Set style for plots\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds and get device\n",
    "set_seeds(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Important**: Make sure these match the settings used during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CONFIGURATION - Must match training settings\n# ============================================\n\n# Dataset configuration\nDATASET_NAME = 'plegma'          # 'refit' or 'plegma'\nAPPLIANCE_NAME = 'boiler'        # Must match training\n\n# Model configuration\nMODEL_NAME = 'cnn'               # 'cnn', 'gru', or 'tcn'\n\n# Model-specific parameters\nMODEL_CONFIGS = {\n    'cnn': {\n        'input_window_length': 299,\n        'batch_size': 1024\n    },\n    'gru': {\n        'input_window_length': 199,\n        'batch_size': 1200\n    },\n    'tcn': {\n        'input_window_length': 600,\n        'batch_size': 50,\n        'depth': 9,\n        'nb_filters': [512, 256, 256, 128, 128, 256, 256, 256, 512],\n        'dropout': 0.2,\n        'stacks': 1\n    }\n}\n\n# REFIT appliance parameters\nREFIT_PARAMS = {\n    'dishwasher': {'threshold': 10, 'cutoff': 2500, 'mean': 602.55, 'std': 828.11},\n    'washing_machine': {'threshold': 20, 'cutoff': 2500, 'mean': 512.30, 'std': 816.25},\n    'kettle': {'threshold': 2000, 'cutoff': 3000, 'mean': 500.10, 'std': 749.24},\n    'microwave': {'threshold': 200, 'cutoff': 1300, 'mean': 489.55, 'std': 696.09},\n    'refrigerator': {'threshold': 5, 'cutoff': 1700, 'mean': 600.18, 'std': 944.55}\n}\n\n# PLEGMA appliance parameters\nPLEGMA_PARAMS = {\n    'ac_1': {'threshold': 50, 'cutoff': 2300, 'mean': 345.71, 'std': 723.03},\n    'boiler': {'threshold': 50, 'cutoff': 5000, 'mean': 347.59, 'std': 745.19},\n    'washing_machine': {'threshold': 50, 'cutoff': 2600, 'mean': 344.49, 'std': 731.61},\n    'fridge': {'threshold': 50, 'cutoff': 400, 'mean': 328.16, 'std': 710.16}\n}\n\n# Select appropriate parameters based on dataset\nAPPLIANCE_PARAMS = REFIT_PARAMS if DATASET_NAME == 'refit' else PLEGMA_PARAMS\n\n# Get appliance parameters\nTHRESHOLD = APPLIANCE_PARAMS[APPLIANCE_NAME]['threshold']\nCUTOFF = APPLIANCE_PARAMS[APPLIANCE_NAME]['cutoff']\n\n# Paths\nDATA_DIR = project_root / 'data' / 'processed' / DATASET_NAME / APPLIANCE_NAME\nOUTPUT_DIR = project_root / 'outputs' / f'{MODEL_NAME}_{APPLIANCE_NAME}'\nCHECKPOINT_PATH = OUTPUT_DIR / 'checkpoint' / 'model.pt'\n\n# Get model-specific config\nmodel_config = MODEL_CONFIGS[MODEL_NAME]\nINPUT_WINDOW_LENGTH = model_config['input_window_length']\nBATCH_SIZE = model_config['batch_size']\n\nprint(f\"Dataset: {DATASET_NAME}\")\nprint(f\"Appliance: {APPLIANCE_NAME}\")\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Input window length: {INPUT_WINDOW_LENGTH}\")\nprint(f\"Threshold: {THRESHOLD} W\")\nprint(f\"Cutoff: {CUTOFF} W\")\nprint(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "if MODEL_NAME == 'cnn':\n",
    "    model = CNN_NILM(input_window_length=INPUT_WINDOW_LENGTH)\n",
    "elif MODEL_NAME == 'gru':\n",
    "    model = GRU_NILM(input_window_length=INPUT_WINDOW_LENGTH)\n",
    "elif MODEL_NAME == 'tcn':\n",
    "    model = TCN_NILM(\n",
    "        input_window_length=INPUT_WINDOW_LENGTH,\n",
    "        depth=model_config.get('depth', 9),\n",
    "        nb_filters=model_config.get('nb_filters'),\n",
    "        dropout=model_config.get('dropout', 0.1),\n",
    "        stacks=model_config.get('stacks', 1)\n",
    "    )\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from: {CHECKPOINT_PATH}\")\n",
    "print(f\"Total parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_loader = SimpleNILMDataLoader(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    model_name=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_window_length=INPUT_WINDOW_LENGTH,\n",
    "    train=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = data_loader.test\n",
    "\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print(f\"Test data shape: {data_loader.test_data.shape}\")\n",
    "print(f\"Test labels shape: {data_loader.test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(model, data_loader, device):\n",
    "    \"\"\"Generate predictions for all data in the loader.\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    for batch_x, _ in tqdm(data_loader, desc=\"Generating predictions\"):\n",
    "        batch_x = batch_x.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(predictions).flatten()\n",
    "\n",
    "# Generate predictions\n",
    "predictions_normalized = predict(model, test_loader, device)\n",
    "print(f\"Predictions shape: {predictions_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align ground truth with predictions based on model type\n",
    "ground_truth_normalized = data_loader.test_labels.copy()\n",
    "\n",
    "if MODEL_NAME == 'cnn':\n",
    "    offset = int(INPUT_WINDOW_LENGTH / 2) - 1\n",
    "    ground_truth_normalized = ground_truth_normalized[offset:]\n",
    "    ground_truth_normalized = ground_truth_normalized[:len(predictions_normalized)]\n",
    "elif MODEL_NAME == 'gru':\n",
    "    offset = INPUT_WINDOW_LENGTH - 1\n",
    "    ground_truth_normalized = ground_truth_normalized[offset:]\n",
    "    ground_truth_normalized = ground_truth_normalized[:len(predictions_normalized)]\n",
    "elif MODEL_NAME == 'tcn':\n",
    "    ground_truth_normalized = ground_truth_normalized[:len(predictions_normalized)]\n",
    "\n",
    "print(f\"Aligned ground truth shape: {ground_truth_normalized.shape}\")\n",
    "print(f\"Predictions shape: {predictions_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize to actual power values\n",
    "ground_truth = ground_truth_normalized * CUTOFF\n",
    "predictions = predictions_normalized * CUTOFF\n",
    "\n",
    "# Apply threshold and clipping\n",
    "predictions_clipped = predictions.copy()\n",
    "predictions_clipped[predictions_clipped < THRESHOLD] = 0\n",
    "predictions_clipped[predictions_clipped > CUTOFF] = CUTOFF\n",
    "\n",
    "print(f\"Ground truth range: [{ground_truth.min():.2f}, {ground_truth.max():.2f}] W\")\n",
    "print(f\"Predictions range: [{predictions_clipped.min():.2f}, {predictions_clipped.max():.2f}] W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(ground_truth, predictions, threshold):\n",
    "    \"\"\"Compute all NILM evaluation metrics.\"\"\"\n",
    "    # MAE\n",
    "    mae = mean_absolute_error(ground_truth, predictions)\n",
    "    \n",
    "    # Binary classification (ON/OFF)\n",
    "    gt_binary = (ground_truth >= threshold).astype(int)\n",
    "    pred_binary = (predictions >= threshold).astype(int)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(gt_binary, pred_binary, labels=[0, 1]).ravel()\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / max(tp + fp, 1e-9)\n",
    "    recall = tp / max(tp + fn, 1e-9)\n",
    "    f1 = 2 * precision * recall / max(precision + recall, 1e-9)\n",
    "    \n",
    "    # Energy metrics\n",
    "    total_gt_energy = np.sum(ground_truth) / 3600  # Wh (assuming 1s sampling)\n",
    "    total_pred_energy = np.sum(predictions) / 3600  # Wh\n",
    "    energy_error = abs(total_gt_energy - total_pred_energy) / max(total_gt_energy, 1e-9) * 100\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'tp': tp,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'total_gt_energy_wh': total_gt_energy,\n",
    "        'total_pred_energy_wh': total_pred_energy,\n",
    "        'energy_error_percent': energy_error\n",
    "    }\n",
    "\n",
    "metrics = compute_metrics(ground_truth, predictions_clipped, THRESHOLD)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean Absolute Error (MAE): {metrics['mae']:.4f} W\")\n",
    "print(f\"\\nClassification Metrics (ON/OFF detection):\")\n",
    "print(f\"  Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {metrics['f1']:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives:  {metrics['tp']:,}\")\n",
    "print(f\"  True Negatives:  {metrics['tn']:,}\")\n",
    "print(f\"  False Positives: {metrics['fp']:,}\")\n",
    "print(f\"  False Negatives: {metrics['fn']:,}\")\n",
    "print(f\"\\nEnergy Metrics:\")\n",
    "print(f\"  Ground Truth Total Energy: {metrics['total_gt_energy_wh']:.2f} Wh\")\n",
    "print(f\"  Predicted Total Energy:    {metrics['total_pred_energy_wh']:.2f} Wh\")\n",
    "print(f\"  Energy Error:              {metrics['energy_error_percent']:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Time Series Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a segment of predictions vs ground truth\n",
    "def plot_predictions(ground_truth, predictions, start_idx=0, length=5000, title=\"\"):\n",
    "    \"\"\"Plot ground truth vs predictions for a segment.\"\"\"\n",
    "    end_idx = min(start_idx + length, len(ground_truth))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    x = np.arange(start_idx, end_idx)\n",
    "    ax.plot(x, ground_truth[start_idx:end_idx], label='Ground Truth', alpha=0.8, linewidth=1)\n",
    "    ax.plot(x, predictions[start_idx:end_idx], label='Prediction', alpha=0.8, linewidth=1)\n",
    "    ax.fill_between(x, 0, ground_truth[start_idx:end_idx], alpha=0.3)\n",
    "    ax.fill_between(x, 0, predictions[start_idx:end_idx], alpha=0.3)\n",
    "    \n",
    "    ax.axhline(y=THRESHOLD, color='r', linestyle='--', label=f'Threshold ({THRESHOLD}W)', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Sample Index')\n",
    "    ax.set_ylabel('Power (W)')\n",
    "    ax.set_title(f'{title} - {APPLIANCE_NAME.capitalize()} Power Prediction')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot several segments\n",
    "fig = plot_predictions(ground_truth, predictions_clipped, start_idx=0, length=5000, title=\"Segment 1\")\n",
    "plt.savefig(OUTPUT_DIR / 'figures' / 'prediction_segment1.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot middle segment\n",
    "mid_point = len(ground_truth) // 2\n",
    "fig = plot_predictions(ground_truth, predictions_clipped, start_idx=mid_point, length=5000, title=\"Segment 2\")\n",
    "plt.savefig(OUTPUT_DIR / 'figures' / 'prediction_segment2.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification\n",
    "gt_binary = (ground_truth >= THRESHOLD).astype(int)\n",
    "pred_binary = (predictions_clipped >= THRESHOLD).astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute values\n",
    "cm = confusion_matrix(gt_binary, pred_binary)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['OFF', 'ON'])\n",
    "disp.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix (Absolute)')\n",
    "\n",
    "# Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=['OFF', 'ON'])\n",
    "disp_norm.plot(ax=axes[1], cmap='Blues', values_format='.2%')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'figures' / 'confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(gt_binary, pred_binary, target_names=['OFF', 'ON']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "errors = predictions_clipped - ground_truth\n",
    "abs_errors = np.abs(errors)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Error histogram\n",
    "axes[0, 0].hist(errors, bins=100, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(x=0, color='r', linestyle='--', label='Zero Error')\n",
    "axes[0, 0].axvline(x=np.mean(errors), color='g', linestyle='--', label=f'Mean: {np.mean(errors):.2f}')\n",
    "axes[0, 0].set_xlabel('Error (W)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Error Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Absolute error histogram\n",
    "axes[0, 1].hist(abs_errors, bins=100, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].axvline(x=np.mean(abs_errors), color='r', linestyle='--', label=f'MAE: {np.mean(abs_errors):.2f}')\n",
    "axes[0, 1].axvline(x=np.median(abs_errors), color='g', linestyle='--', label=f'Median: {np.median(abs_errors):.2f}')\n",
    "axes[0, 1].set_xlabel('Absolute Error (W)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Absolute Error Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Scatter plot: Ground Truth vs Predictions\n",
    "sample_indices = np.random.choice(len(ground_truth), min(10000, len(ground_truth)), replace=False)\n",
    "axes[1, 0].scatter(ground_truth[sample_indices], predictions_clipped[sample_indices], alpha=0.3, s=1)\n",
    "axes[1, 0].plot([0, CUTOFF], [0, CUTOFF], 'r--', label='Perfect Prediction')\n",
    "axes[1, 0].set_xlabel('Ground Truth (W)')\n",
    "axes[1, 0].set_ylabel('Prediction (W)')\n",
    "axes[1, 0].set_title('Ground Truth vs Predictions (Sampled)')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Box plot of errors by power level\n",
    "power_bins = pd.cut(ground_truth, bins=[0, THRESHOLD, CUTOFF/4, CUTOFF/2, CUTOFF], \n",
    "                    labels=['Off', 'Low', 'Medium', 'High'])\n",
    "error_df = pd.DataFrame({'Power Level': power_bins, 'Error': abs_errors})\n",
    "error_df.boxplot(column='Error', by='Power Level', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Power Level')\n",
    "axes[1, 1].set_ylabel('Absolute Error (W)')\n",
    "axes[1, 1].set_title('Error Distribution by Power Level')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'figures' / 'error_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print error statistics\n",
    "print(\"\\nError Statistics:\")\n",
    "print(f\"  Mean Error: {np.mean(errors):.4f} W\")\n",
    "print(f\"  Std Error: {np.std(errors):.4f} W\")\n",
    "print(f\"  MAE: {np.mean(abs_errors):.4f} W\")\n",
    "print(f\"  Median Absolute Error: {np.median(abs_errors):.4f} W\")\n",
    "print(f\"  Max Absolute Error: {np.max(abs_errors):.4f} W\")\n",
    "print(f\"  90th Percentile Error: {np.percentile(abs_errors, 90):.4f} W\")\n",
    "print(f\"  95th Percentile Error: {np.percentile(abs_errors, 95):.4f} W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Error Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling MAE\n",
    "window_size = 1000\n",
    "rolling_mae = pd.Series(abs_errors).rolling(window=window_size).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "ax.plot(rolling_mae, alpha=0.8, linewidth=0.5)\n",
    "ax.axhline(y=np.mean(abs_errors), color='r', linestyle='--', label=f'Overall MAE: {np.mean(abs_errors):.2f} W')\n",
    "ax.fill_between(range(len(rolling_mae)), 0, rolling_mae, alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Rolling MAE (W)')\n",
    "ax.set_title(f'Rolling MAE (Window={window_size})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'figures' / 'rolling_mae.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "(OUTPUT_DIR / 'metrics').mkdir(exist_ok=True)\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df = pd.DataFrame([{\n",
    "    'Model': MODEL_NAME,\n",
    "    'Appliance': APPLIANCE_NAME,\n",
    "    'Dataset': DATASET_NAME,\n",
    "    'MAE': round(metrics['mae'], 4),\n",
    "    'F1': round(metrics['f1'], 4),\n",
    "    'Accuracy': round(metrics['accuracy'], 4),\n",
    "    'Precision': round(metrics['precision'], 4),\n",
    "    'Recall': round(metrics['recall'], 4),\n",
    "    'Energy_Error_Percent': round(metrics['energy_error_percent'], 2)\n",
    "}])\n",
    "\n",
    "metrics_path = OUTPUT_DIR / 'metrics' / f'{APPLIANCE_NAME}_results.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"Metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Display metrics table\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for further analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ground_truth': ground_truth,\n",
    "    'prediction': predictions_clipped,\n",
    "    'error': errors,\n",
    "    'abs_error': abs_errors\n",
    "})\n",
    "\n",
    "predictions_path = OUTPUT_DIR / 'metrics' / f'{APPLIANCE_NAME}_predictions.csv'\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "print(f\"Predictions saved to: {predictions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Model:          {MODEL_NAME.upper()}\")\n",
    "print(f\"  Appliance:      {APPLIANCE_NAME}\")\n",
    "print(f\"  Dataset:        {DATASET_NAME}\")\n",
    "print(f\"  Window Length:  {INPUT_WINDOW_LENGTH}\")\n",
    "print(f\"  Parameters:     {count_parameters(model):,}\")\n",
    "print(f\"\\nRegression Metrics:\")\n",
    "print(f\"  MAE:            {metrics['mae']:.4f} W\")\n",
    "print(f\"  Energy Error:   {metrics['energy_error_percent']:.2f}%\")\n",
    "print(f\"\\nClassification Metrics (Threshold: {THRESHOLD}W):\")\n",
    "print(f\"  Accuracy:       {metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"  F1 Score:       {metrics['f1']:.4f}\")\n",
    "print(f\"  Precision:      {metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:         {metrics['recall']:.4f}\")\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Metrics:     {OUTPUT_DIR / 'metrics'}\")\n",
    "print(f\"  Figures:     {OUTPUT_DIR / 'figures'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Multiple Models (Optional)\n",
    "\n",
    "If you have trained multiple models, you can compare them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load and compare results from multiple models\n",
    "def load_results(model_name, appliance_name):\n",
    "    \"\"\"Load results for a specific model-appliance combination.\"\"\"\n",
    "    results_path = project_root / 'outputs' / f'{model_name}_{appliance_name}' / 'metrics' / f'{appliance_name}_results.csv'\n",
    "    if results_path.exists():\n",
    "        return pd.read_csv(results_path)\n",
    "    return None\n",
    "\n",
    "# Try to load results for all models\n",
    "all_results = []\n",
    "for model in ['cnn', 'gru', 'tcn']:\n",
    "    result = load_results(model, APPLIANCE_NAME)\n",
    "    if result is not None:\n",
    "        all_results.append(result)\n",
    "\n",
    "if len(all_results) > 1:\n",
    "    comparison_df = pd.concat(all_results, ignore_index=True)\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    comparison_df.plot(x='Model', y='MAE', kind='bar', ax=axes[0], legend=False, color='steelblue')\n",
    "    axes[0].set_ylabel('MAE (W)')\n",
    "    axes[0].set_title('MAE Comparison')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    comparison_df.plot(x='Model', y='F1', kind='bar', ax=axes[1], legend=False, color='darkorange')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].set_title('F1 Score Comparison')\n",
    "    axes[1].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'figures' / 'model_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Only one model result found. Train more models to enable comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}