{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ENERGIZE NILM — Structured Pruning, Fine-tuning & Evaluation\n",
    "\n",
    "This notebook supports **CNN** and **TCN** models and any PLEGMA appliance.\n",
    "\n",
    "Pipeline:\n",
    "1. **Configure** — choose model, appliance and pruning ratio in one cell\n",
    "2. **Baseline** — load the trained checkpoint, measure cost, evaluate on test set\n",
    "3. **Prune** — apply global structured channel pruning (50% by default)\n",
    "4. **Evaluate pruned** — test-set metrics immediately after pruning\n",
    "5. **Fine-tune** — 1-epoch recovery training on the training set\n",
    "6. **Evaluate fine-tuned** — test-set metrics after fine-tuning\n",
    "7. **Export** — save all results (Params, MACs, MB + metrics) to Excel\n",
    "\n",
    "> Pruning functions live in `src_pytorch/pruner.py` and are imported here.\n",
    "\n",
    "---\n",
    "\n",
    "## Google Colab Setup\n",
    "1. Upload your `ENERGIZE` folder to Google Drive\n",
    "2. Run the cell below first and edit `DRIVE_PROJECT_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLAB SETUP — run this cell first\n",
    "# ============================================================================\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'torch_pruning', 'openpyxl'])\n",
    "\n",
    "    # =========================================================================\n",
    "    DRIVE_PROJECT_PATH = '/content/drive/MyDrive/ENERGIZE'  # <-- EDIT THIS\n",
    "    # =========================================================================\n",
    "\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    project_root = Path(DRIVE_PROJECT_PATH)\n",
    "    if not project_root.exists():\n",
    "        raise FileNotFoundError(f\"Project folder not found: {project_root}\")\n",
    "    os.chdir(project_root)\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Project root: {project_root}\")\n",
    "else:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    project_root = Path(os.getcwd()).parent\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Running locally. Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_pruning as tp\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NILM package\n",
    "from src_pytorch import (\n",
    "    CNN_NILM, TCN_NILM,\n",
    "    SimpleNILMDataLoader,\n",
    "    set_seeds, get_device, count_parameters,\n",
    "    # Pruning utilities (src_pytorch/pruner.py)\n",
    "    count_parameters_per_layer,\n",
    "    get_model_stats,\n",
    "    apply_torch_pruning,\n",
    "    evaluate_model,\n",
    ")\n",
    "\n",
    "set_seeds(42)\n",
    "device = get_device()\n",
    "\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"torch_pruning   : {tp.__version__}\")\n",
    "print(f\"Device          : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "**Edit only this cell.** Everything else adapts automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# USER CONFIGURATION\n# ============================================================================\nMODEL_NAME     = 'tcn'      # 'cnn'  |  'tcn'\nAPPLIANCE_NAME = 'boiler'   # 'boiler'  |  'ac_1'  |  'washing_machine'\nPRUNING_RATIO  = 0.5        # fraction of channels to remove  (0.0 – 1.0)\nFINETUNE_LR    = 1e-4       # Adam learning rate for 1-epoch fine-tuning\nDATASET_NAME   = 'plegma'\n\n# ============================================================================\n# AUTO-DERIVED — do not edit below this line\n# ============================================================================\n\n# Model-specific architecture parameters\n_MODEL_CFGS = {\n    'cnn': {\n        'window'          : 299,\n        'batch_size'      : 1024,\n        'args_window_size': 1,      # out_features of final Linear — protects it\n    },\n    'tcn': {\n        'window'          : 600,\n        'batch_size'      : 50,\n        'depth'           : 9,\n        'filters'         : [512, 256, 256, 128, 128, 256, 256, 256, 512],\n        'dropout'         : 0.2,\n        'stacks'          : 1,\n        'args_window_size': 600,    # no Linear layers in TCN, kept for clarity\n    },\n}\n\n# PLEGMA appliance thresholds and cutoffs\n_APPLIANCE_CFGS = {\n    'boiler'          : {'threshold': 50, 'cutoff': 5000},\n    'ac_1'            : {'threshold': 50, 'cutoff': 2300},\n    'washing_machine' : {'threshold': 50, 'cutoff': 2600},\n}\n\ncfg     = _MODEL_CFGS[MODEL_NAME]\napp_cfg = _APPLIANCE_CFGS[APPLIANCE_NAME]\n\nWINDOW     = cfg['window']\nBATCH_SIZE = cfg['batch_size']\nTHRESHOLD  = app_cfg['threshold']\nCUTOFF     = app_cfg['cutoff']\n\nDATA_DIR    = project_root / 'data' / 'processed' / DATASET_NAME / APPLIANCE_NAME\nCKPT_PATH   = project_root / 'outputs' / f'{MODEL_NAME}_{APPLIANCE_NAME}' / 'checkpoint' / 'model.pt'\n# Results saved alongside the model outputs for this experiment\nRESULTS_DIR = project_root / 'outputs' / f'{MODEL_NAME}_{APPLIANCE_NAME}' / 'comparative_results'\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Model          : {MODEL_NAME.upper()}\")\nprint(f\"Appliance      : {APPLIANCE_NAME}\")\nprint(f\"Pruning ratio  : {PRUNING_RATIO * 100:.0f}%\")\nprint(f\"Fine-tune LR   : {FINETUNE_LR}\")\nprint(f\"Window length  : {WINDOW}\")\nprint(f\"Threshold      : {THRESHOLD} W  |  Cutoff: {CUTOFF} W\")\nprint(f\"Checkpoint     : {CKPT_PATH}  ({'found' if CKPT_PATH.exists() else 'NOT FOUND'})\")\nprint(f\"Results dir    : {RESULTS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = SimpleNILMDataLoader(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    model_name=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_window_length=WINDOW,\n",
    "    train=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches : {len(data_loader.train)}\")\n",
    "print(f\"Val   batches : {len(data_loader.val)}\")\n",
    "print(f\"Test  batches : {len(data_loader.test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Helper — Model Factory\n",
    "\n",
    "A single function that builds and loads the correct architecture from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name: str, cfg: dict, ckpt_path, device) -> nn.Module:\n",
    "    \"\"\"Instantiate and load a NILM model from a checkpoint.\"\"\"\n",
    "    if model_name == 'cnn':\n",
    "        model = CNN_NILM(input_window_length=cfg['window'])\n",
    "    elif model_name == 'tcn':\n",
    "        model = TCN_NILM(\n",
    "            input_window_length=cfg['window'],\n",
    "            depth=cfg['depth'],\n",
    "            nb_filters=cfg['filters'],\n",
    "            dropout=cfg['dropout'],\n",
    "            stacks=cfg['stacks'],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}. Choose 'cnn' or 'tcn'.\")\n",
    "\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    return model.to(device).eval()\n",
    "\n",
    "\n",
    "print(\"build_model() defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Baseline — Load & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model\n",
    "baseline_model = build_model(MODEL_NAME, cfg, CKPT_PATH, device)\n",
    "dummy_input    = torch.randn(1, WINDOW).to(device)\n",
    "\n",
    "baseline_params, baseline_macs, baseline_mb = get_model_stats(baseline_model, dummy_input)\n",
    "\n",
    "print(f\"Model      : {MODEL_NAME.upper()}\")\n",
    "print(f\"Parameters : {baseline_params:,}\")\n",
    "print(f\"MACs       : {baseline_macs:,}\")\n",
    "print(f\"Size       : {baseline_mb:.3f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline on test set\n",
    "print(f\"Evaluating {MODEL_NAME.upper()} baseline on test set...\")\n",
    "baseline_metrics = evaluate_model(\n",
    "    model=baseline_model,\n",
    "    data_loader=data_loader,\n",
    "    model_name=MODEL_NAME,\n",
    "    cutoff=CUTOFF,\n",
    "    threshold=THRESHOLD,\n",
    "    device=device,\n",
    "    input_window_length=WINDOW,\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "for k, v in baseline_metrics.items():\n",
    "    print(f\"  {k:<25}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Prune & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload a fresh instance — pruning is irreversible\n",
    "pruned_model = build_model(MODEL_NAME, cfg, CKPT_PATH, device)\n",
    "pruning_args = SimpleNamespace(window_size=cfg['args_window_size'])\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Pruning {MODEL_NAME.upper()} at {PRUNING_RATIO * 100:.0f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pruned_model, _ = apply_torch_pruning(\n",
    "    model=pruned_model,\n",
    "    args=pruning_args,\n",
    "    inputs=dummy_input,\n",
    "    pruning_ratio=PRUNING_RATIO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model cost after pruning\n",
    "pruned_params, pruned_macs, pruned_mb = get_model_stats(pruned_model, dummy_input)\n",
    "\n",
    "print(f\"Parameters : {pruned_params:,}  ({(1 - pruned_params/baseline_params)*100:.1f}% reduction)\")\n",
    "print(f\"MACs       : {pruned_macs:,}  ({(1 - pruned_macs/baseline_macs)*100:.1f}% reduction)\")\n",
    "print(f\"Size       : {pruned_mb:.3f} MB  ({(1 - pruned_mb/baseline_mb)*100:.1f}% reduction)\")\n",
    "\n",
    "print(\"\\nPer-layer parameter counts (after pruning):\")\n",
    "for name, cnt in count_parameters_per_layer(pruned_model).items():\n",
    "    print(f\"  {name:<60} {cnt:>10,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pruned model\n",
    "print(f\"Evaluating pruned {MODEL_NAME.upper()} on test set...\")\n",
    "pruned_metrics = evaluate_model(\n",
    "    model=pruned_model,\n",
    "    data_loader=data_loader,\n",
    "    model_name=MODEL_NAME,\n",
    "    cutoff=CUTOFF,\n",
    "    threshold=THRESHOLD,\n",
    "    device=device,\n",
    "    input_window_length=WINDOW,\n",
    ")\n",
    "\n",
    "print(f\"\\nPruned Results:\")\n",
    "for k, v in pruned_metrics.items():\n",
    "    print(f\"  {k:<25}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pruned checkpoint\n",
    "pruned_ckpt = RESULTS_DIR / f'{MODEL_NAME}_{APPLIANCE_NAME}_pruned_{int(PRUNING_RATIO*100)}pct.pt'\n",
    "torch.save(pruned_model.state_dict(), pruned_ckpt)\n",
    "print(f\"Pruned checkpoint saved: {pruned_ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Fine-tune (1 epoch)\n",
    "\n",
    "One epoch of MSE training on the training set with Adam at `FINETUNE_LR`.\n",
    "The pruned model structure is preserved — no re-growing of pruned channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(pruned_model.parameters(), lr=FINETUNE_LR)\n",
    "loss_fn   = nn.MSELoss()\n",
    "\n",
    "pruned_model.train()\n",
    "total_loss  = 0.0\n",
    "total_mae   = 0.0\n",
    "n_batches   = 0\n",
    "\n",
    "for batch_x, batch_y in tqdm(data_loader.train, desc=\"Fine-tuning epoch 1\"):\n",
    "    batch_x = batch_x.to(device)\n",
    "    batch_y = batch_y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = pruned_model(batch_x)\n",
    "\n",
    "    # Align target shape to model output\n",
    "    # CNN  : output (B, 1),          target (B,)       → unsqueeze to (B, 1)\n",
    "    # TCN  : output (B, seq_len, 1), target (B, seq_len) → unsqueeze to (B, seq_len, 1)\n",
    "    if MODEL_NAME == 'cnn' and batch_y.dim() == 1:\n",
    "        batch_y = batch_y.unsqueeze(1)\n",
    "    elif MODEL_NAME == 'tcn' and batch_y.dim() == 2:\n",
    "        batch_y = batch_y.unsqueeze(-1)\n",
    "\n",
    "    loss = loss_fn(outputs, batch_y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_mae  += torch.mean(torch.abs(outputs.detach() - batch_y)).item()\n",
    "    n_batches  += 1\n",
    "\n",
    "pruned_model.eval()\n",
    "print(f\"\\nFine-tune epoch 1 — avg MSE loss: {total_loss/n_batches:.6f}  \"\n",
    "      f\"avg MAE: {total_mae/n_batches:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned checkpoint\n",
    "finetuned_ckpt = RESULTS_DIR / f'{MODEL_NAME}_{APPLIANCE_NAME}_pruned_{int(PRUNING_RATIO*100)}pct_finetuned.pt'\n",
    "torch.save(pruned_model.state_dict(), finetuned_ckpt)\n",
    "print(f\"Fine-tuned checkpoint saved: {finetuned_ckpt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 8. Evaluate Fine-tuned Model\n",
    "\n",
    "Architecture (and therefore Params / MACs / MB) is unchanged by fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_params, finetuned_macs, finetuned_mb = get_model_stats(pruned_model, dummy_input)\n",
    "\n",
    "print(f\"Evaluating fine-tuned {MODEL_NAME.upper()} on test set...\")\n",
    "finetuned_metrics = evaluate_model(\n",
    "    model=pruned_model,\n",
    "    data_loader=data_loader,\n",
    "    model_name=MODEL_NAME,\n",
    "    cutoff=CUTOFF,\n",
    "    threshold=THRESHOLD,\n",
    "    device=device,\n",
    "    input_window_length=WINDOW,\n",
    ")\n",
    "\n",
    "print(f\"\\nFine-tuned Results:\")\n",
    "for k, v in finetuned_metrics.items():\n",
    "    print(f\"  {k:<25}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Export Results to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_row(label, pruning_pct, params, macs, mb, metrics):\n",
    "    return {\n",
    "        'Model'           : label,\n",
    "        'Architecture'    : MODEL_NAME.upper(),\n",
    "        'Appliance'       : APPLIANCE_NAME,\n",
    "        'Pruning_Ratio_%' : pruning_pct,\n",
    "        'Params'          : params,\n",
    "        'MACs'            : macs,\n",
    "        'MB'              : mb,\n",
    "        'MAE'             : round(metrics['mae'],                  4),\n",
    "        'F1'              : round(metrics['f1'],                   4),\n",
    "        'Precision'       : round(metrics['precision'],            4),\n",
    "        'Recall'          : round(metrics['recall'],               4),\n",
    "        'Accuracy'        : round(metrics['accuracy'],             4),\n",
    "        'Energy_Error_%'  : round(metrics['energy_error_percent'], 2),\n",
    "    }\n",
    "\n",
    "\n",
    "pct = int(PRUNING_RATIO * 100)\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "    make_row(f'{MODEL_NAME.upper()} Baseline',\n",
    "             0, baseline_params, baseline_macs, baseline_mb, baseline_metrics),\n",
    "    make_row(f'{MODEL_NAME.upper()} Pruned {pct}%',\n",
    "             pct, pruned_params, pruned_macs, pruned_mb, pruned_metrics),\n",
    "    make_row(f'{MODEL_NAME.upper()} Pruned {pct}% + Fine-tuned 1ep',\n",
    "             pct, finetuned_params, finetuned_macs, finetuned_mb, finetuned_metrics),\n",
    "])\n",
    "\n",
    "excel_path = RESULTS_DIR / f'{MODEL_NAME}_{APPLIANCE_NAME}_pruning_results.xlsx'\n",
    "results_df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"Results saved to: {excel_path}\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "C, W3 = 22, 16\n",
    "SEP   = \"=\" * (C + W3 * 3 + 3)\n",
    "sep   = \"-\" * (C + W3 * 3 + 3)\n",
    "hfmt  = f\"{{:<{C}}} {{:>{W3}}} {{:>{W3}}} {{:>{W3}}}\"\n",
    "rfmt  = f\"{{:<{C}}} {{:>{W3}}} {{:>{W3}}} {{:>{W3}}}\"\n",
    "\n",
    "col_baseline  = f\"{MODEL_NAME.upper()} Baseline\"\n",
    "col_pruned    = f\"Pruned {pct}%\"\n",
    "col_finetuned = f\"Pruned+FT 1ep\"\n",
    "\n",
    "print(SEP)\n",
    "print(f\"PRUNING SUMMARY  |  {MODEL_NAME.upper()}  |  {APPLIANCE_NAME}  |  pruning={pct}%  |  LR={FINETUNE_LR}\")\n",
    "print(SEP)\n",
    "print(hfmt.format('Metric', col_baseline, col_pruned, col_finetuned))\n",
    "print(sep)\n",
    "\n",
    "def rrow(label, b, p, f):\n",
    "    print(rfmt.format(label, b, p, f))\n",
    "\n",
    "rrow('Params',        f\"{baseline_params:,}\",   f\"{pruned_params:,}\",   f\"{finetuned_params:,}\")\n",
    "rrow('MACs',          f\"{baseline_macs:,}\",     f\"{pruned_macs:,}\",     f\"{finetuned_macs:,}\")\n",
    "rrow('MB',            f\"{baseline_mb:.3f}\",     f\"{pruned_mb:.3f}\",     f\"{finetuned_mb:.3f}\")\n",
    "print(sep)\n",
    "rrow('MAE (W)',       f\"{baseline_metrics['mae']:.4f}\",\n",
    "                      f\"{pruned_metrics['mae']:.4f}\",\n",
    "                      f\"{finetuned_metrics['mae']:.4f}\")\n",
    "rrow('F1',            f\"{baseline_metrics['f1']:.4f}\",\n",
    "                      f\"{pruned_metrics['f1']:.4f}\",\n",
    "                      f\"{finetuned_metrics['f1']:.4f}\")\n",
    "rrow('Precision',     f\"{baseline_metrics['precision']:.4f}\",\n",
    "                      f\"{pruned_metrics['precision']:.4f}\",\n",
    "                      f\"{finetuned_metrics['precision']:.4f}\")\n",
    "rrow('Recall',        f\"{baseline_metrics['recall']:.4f}\",\n",
    "                      f\"{pruned_metrics['recall']:.4f}\",\n",
    "                      f\"{finetuned_metrics['recall']:.4f}\")\n",
    "rrow('Accuracy',      f\"{baseline_metrics['accuracy']:.4f}\",\n",
    "                      f\"{pruned_metrics['accuracy']:.4f}\",\n",
    "                      f\"{finetuned_metrics['accuracy']:.4f}\")\n",
    "rrow('Energy Err %',  f\"{baseline_metrics['energy_error_percent']:.2f}\",\n",
    "                      f\"{pruned_metrics['energy_error_percent']:.2f}\",\n",
    "                      f\"{finetuned_metrics['energy_error_percent']:.2f}\")\n",
    "print(SEP)\n",
    "print(f\"Excel  : {excel_path}\")\n",
    "print(f\"Ckpts  : {pruned_ckpt.name}\")\n",
    "print(f\"         {finetuned_ckpt.name}\")\n",
    "print(SEP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}