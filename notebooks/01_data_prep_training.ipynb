{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenNILM - Data Preparation & Training (PyTorch)\n",
    "\n",
    "This notebook covers:\n",
    "1. **Data Preparation**: Loading and preprocessing NILM datasets (REFIT/PLEGMA)\n",
    "2. **Model Configuration**: Setting up CNN, GRU, or TCN models\n",
    "3. **Training**: Training the model with early stopping and checkpointing\n",
    "4. **Visualization**: Training curves and model analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Google Colab Setup\n",
    "\n",
    "**If running on Colab:**\n",
    "1. Upload your `OpenNILM` folder to Google Drive (e.g., `My Drive/OpenNILM/`)\n",
    "2. Run the Colab setup cells below first\n",
    "3. Edit `DRIVE_PROJECT_PATH` to match your folder location\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COLAB SETUP - Run this cell first!\n",
    "# ============================================================================\n",
    "import sys\n",
    "\n",
    "# Detect if running on Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "    # CONFIGURE YOUR GOOGLE DRIVE PATH HERE\n",
    "    # =========================================================================\n",
    "    DRIVE_PROJECT_PATH = '/content/drive/MyDrive/ENERGIZE'  # <-- EDIT THIS PATH\n",
    "    # =========================================================================\n",
    "    \n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    project_root = Path(DRIVE_PROJECT_PATH)\n",
    "    \n",
    "    if not project_root.exists():\n",
    "        print(f\"ERROR: Project folder not found at: {project_root}\")\n",
    "        print(f\"Please upload OpenNILM to Google Drive or edit DRIVE_PROJECT_PATH above\")\n",
    "        print(f\"\\nYour Drive contents:\")\n",
    "        !ls \"/content/drive/MyDrive/\" | head -15\n",
    "    else:\n",
    "        os.chdir(project_root)\n",
    "        sys.path.insert(0, str(project_root))\n",
    "        print(f\"Project root: {project_root}\")\n",
    "        print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    project_root = Path(os.getcwd()).parent\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Running locally. Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS (os, sys, Path, project_root already defined in Colab setup cell)\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# PyTorch NILM modules\n",
    "from src_pytorch import (\n",
    "    CNN_NILM, GRU_NILM, TCN_NILM, get_model,\n",
    "    SimpleNILMDataLoader,\n",
    "    Trainer, EarlyStopping, ModelCheckpoint, TrainingHistory,\n",
    "    set_seeds, get_device, count_parameters, print_model_summary,\n",
    "    # Config\n",
    "    MODEL_CONFIGS, TRAINING, get_appliance_params, get_model_config\n",
    ")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(42)\n",
    "\n",
    "# Get device\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure the experiment parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USER CONFIGURATION - Modify these values only\n",
    "# ============================================================================\n",
    "DATASET_NAME = 'plegma'      # 'refit' or 'plegma'\n",
    "APPLIANCE_NAME = 'boiler'    # REFIT: dishwasher, washing_machine, kettle, microwave, refrigerator\n",
    "                             # PLEGMA: ac_1, boiler, washing_machine, fridge\n",
    "MODEL_NAME = 'tcn'           # 'cnn', 'gru', or 'tcn'\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-LOADED FROM CONFIG (src_pytorch/config.py) - Don't modify below\n",
    "# ============================================================================\n",
    "# Get model configuration\n",
    "model_config = get_model_config(MODEL_NAME)\n",
    "INPUT_WINDOW_LENGTH = model_config['input_window_length']\n",
    "BATCH_SIZE = model_config['batch_size']\n",
    "\n",
    "# Get appliance parameters\n",
    "appliance_params = get_appliance_params(DATASET_NAME, APPLIANCE_NAME)\n",
    "THRESHOLD = appliance_params['threshold']\n",
    "CUTOFF = appliance_params['cutoff']\n",
    "AGG_MEAN = appliance_params['mean']\n",
    "AGG_STD = appliance_params['std']\n",
    "\n",
    "# Get training parameters\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = TRAINING['learning_rate']\n",
    "EARLY_STOPPING_PATIENCE = TRAINING['early_stopping_patience']\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = project_root / 'data' / 'processed' / DATASET_NAME / APPLIANCE_NAME\n",
    "OUTPUT_DIR = project_root / 'outputs' / f'{MODEL_NAME}_{APPLIANCE_NAME}'\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / 'checkpoint').mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / 'tensorboard').mkdir(exist_ok=True)\n",
    "(OUTPUT_DIR / 'figures').mkdir(exist_ok=True)\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset:           {DATASET_NAME}\")\n",
    "print(f\"Appliance:         {APPLIANCE_NAME}\")\n",
    "print(f\"Model:             {MODEL_NAME}\")\n",
    "print(f\"Window length:     {INPUT_WINDOW_LENGTH}\")\n",
    "print(f\"Batch size:        {BATCH_SIZE}\")\n",
    "print(f\"Epochs:            {EPOCHS}\")\n",
    "print(f\"Learning rate:     {LEARNING_RATE}\")\n",
    "print(f\"Threshold:         {THRESHOLD} W\")\n",
    "print(f\"Cutoff:            {CUTOFF} W\")\n",
    "print(f\"Data directory:    {DATA_DIR}\")\n",
    "print(f\"Output directory:  {OUTPUT_DIR}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "### 3.1 Load Raw Data (Optional - For Data Exploration)\n",
    "\n",
    "If you need to process raw data first, run the data processing script:\n",
    "```bash\n",
    "cd data\n",
    "python data.py dataset=refit appliance=dishwasher\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if processed data exists\n",
    "if not DATA_DIR.exists():\n",
    "    print(f\"Warning: Data directory does not exist: {DATA_DIR}\")\n",
    "    print(\"Please run the data processing script first.\")\n",
    "else:\n",
    "    print(f\"Data directory found: {DATA_DIR}\")\n",
    "    print(f\"Files: {list(DATA_DIR.glob('*.csv'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Explore the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the training data\n",
    "train_df = pd.read_csv(DATA_DIR / 'training_.csv')\n",
    "val_df = pd.read_csv(DATA_DIR / 'validation_.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'test_.csv')\n",
    "\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Validation data shape:\", val_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "print(\"\\nColumn names:\", train_df.columns.tolist())\n",
    "print(\"\\nTraining data statistics:\")\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample of the data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "sample_size = min(10000, len(train_df))\n",
    "sample = train_df.iloc[:sample_size]\n",
    "\n",
    "axes[0].plot(sample.iloc[:, 0], label='Aggregate Power (normalized)', alpha=0.8)\n",
    "axes[0].set_ylabel('Normalized Power')\n",
    "axes[0].set_title('Aggregate Power')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(sample.iloc[:, 1], label=f'{APPLIANCE_NAME} Power (normalized)', alpha=0.8, color='orange')\n",
    "axes[1].set_ylabel('Normalized Power')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_title(f'{APPLIANCE_NAME} Power')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'figures' / 'data_visualization.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "data_loader = SimpleNILMDataLoader(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    model_name=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    input_window_length=INPUT_WINDOW_LENGTH,\n",
    "    train=True,\n",
    "    num_workers=0  # Set to > 0 for parallel data loading\n",
    ")\n",
    "\n",
    "# Get data loaders\n",
    "train_loader = data_loader.train\n",
    "val_loader = data_loader.val\n",
    "test_loader = data_loader.test\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Check a batch\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(f\"\\nBatch X shape: {batch_x.shape}\")\n",
    "print(f\"Batch Y shape: {batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "if MODEL_NAME == 'cnn':\n",
    "    model = CNN_NILM(input_window_length=INPUT_WINDOW_LENGTH)\n",
    "elif MODEL_NAME == 'gru':\n",
    "    model = GRU_NILM(input_window_length=INPUT_WINDOW_LENGTH)\n",
    "elif MODEL_NAME == 'tcn':\n",
    "    model = TCN_NILM(\n",
    "        input_window_length=INPUT_WINDOW_LENGTH,\n",
    "        depth=model_config.get('depth', 9),\n",
    "        nb_filters=model_config.get('nb_filters'),\n",
    "        dropout=model_config.get('dropout', 0.1),\n",
    "        stacks=model_config.get('stacks', 1)\n",
    "    )\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"\\nModel: {MODEL_NAME.upper()}\")\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n",
    "print(\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_input = batch_x[:2].to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"Test input shape: {test_input.shape}\")\n",
    "    print(f\"Test output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "trainer.setup_callbacks(\n",
    "    checkpoint_dir=str(OUTPUT_DIR / 'checkpoint'),\n",
    "    tensorboard_dir=str(OUTPUT_DIR / 'tensorboard'),\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    early_stopping_min_delta=1e-6\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")\n",
    "print(f\"Checkpoint will be saved to: {OUTPUT_DIR / 'checkpoint' / 'model.pt'}\")\n",
    "print(f\"TensorBoard logs will be saved to: {OUTPUT_DIR / 'tensorboard'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 151/790 [07:55<32:40,  3.07s/it, loss=0.000233]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(f\"\\nStarting training for {EPOCHS} epochs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(history.epochs, history.train_loss, label='Training Loss', marker='o', markersize=3)\n",
    "axes[0].plot(history.epochs, history.val_loss, label='Validation Loss', marker='o', markersize=3)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE plot\n",
    "if history.train_mae:\n",
    "    axes[1].plot(history.epochs, history.train_mae, label='Training MAE', marker='o', markersize=3)\n",
    "    axes[1].plot(history.epochs, history.val_mae, label='Validation MAE', marker='o', markersize=3)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE')\n",
    "    axes[1].set_title('Training and Validation MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'figures' / 'training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print best results\n",
    "best_epoch = np.argmin(history.val_loss)\n",
    "print(f\"\\nBest epoch: {best_epoch + 1}\")\n",
    "print(f\"Best validation loss: {history.val_loss[best_epoch]:.6f}\")\n",
    "print(f\"Best validation MAE: {history.val_mae[best_epoch]:.6f}\" if history.val_mae else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_df = pd.DataFrame({\n",
    "    'epoch': history.epochs,\n",
    "    'train_loss': history.train_loss,\n",
    "    'val_loss': history.val_loss,\n",
    "    'train_mae': history.train_mae,\n",
    "    'val_mae': history.val_mae\n",
    "})\n",
    "history_df.to_csv(OUTPUT_DIR / 'training_history.csv', index=False)\n",
    "print(f\"Training history saved to: {OUTPUT_DIR / 'training_history.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Best Model and Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "checkpoint_path = OUTPUT_DIR / 'checkpoint' / 'model.pt'\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded best model from: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation on validation set\n",
    "@torch.no_grad()\n",
    "def quick_evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mae = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_x, batch_y in data_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        if outputs.shape != batch_y.shape:\n",
    "            if outputs.dim() == 2 and batch_y.dim() == 1:\n",
    "                batch_y = batch_y.unsqueeze(1)\n",
    "        \n",
    "        loss = nn.MSELoss()(outputs, batch_y)\n",
    "        mae = torch.mean(torch.abs(outputs - batch_y))\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mae.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches, total_mae / num_batches\n",
    "\n",
    "val_loss, val_mae = quick_evaluate(model, val_loader, device)\n",
    "test_loss, test_mae = quick_evaluate(model, test_loader, device)\n",
    "\n",
    "print(f\"Validation - Loss: {val_loss:.6f}, MAE: {val_mae:.6f}\")\n",
    "print(f\"Test - Loss: {test_loss:.6f}, MAE: {test_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Training is complete! The model has been saved to the checkpoint directory.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Open `02_evaluation.ipynb` for detailed evaluation and visualization\n",
    "2. Launch TensorBoard to view training logs:\n",
    "   ```bash\n",
    "   tensorboard --logdir outputs/{model}_{appliance}/tensorboard\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Appliance: {APPLIANCE_NAME}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Total parameters: {count_parameters(model):,}\")\n",
    "print(f\"Epochs trained: {len(history.epochs)}\")\n",
    "print(f\"Best validation loss: {min(history.val_loss):.6f}\")\n",
    "print(f\"Test loss: {test_loss:.6f}\")\n",
    "print(f\"Test MAE: {test_mae:.6f}\")\n",
    "print(f\"\\nCheckpoint saved to: {checkpoint_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
